export default async function handler(req, res) {
  console.log('ü§ñ Save & Taste API Iniciada');
  
  // Headers CORS
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');
  
  // Manejar preflight
  if (req.method === 'OPTIONS') {
    return res.status(200).end();
  }
  
  // Solo POST
  if (req.method !== 'POST') {
    return res.status(405).json({ 
      success: false, 
      error: 'M√©todo no permitido' 
    });
  }

  try {
    const { food, option, isSpoiled } = req.body;
    
    // Validaci√≥n b√°sica
    if (!food || !option) {
      return res.status(400).json({ 
        success: false, 
        error: 'Faltan campos obligatorios' 
      });
    }

    console.log('üì• Datos recibidos:', { food, option, isSpoiled });
    
    // ============================================
    // VERIFICACI√ìN DEL TOKEN
    // ============================================
    const HF_TOKEN = process.env.HF_TOKEN;
    
    if (!HF_TOKEN) {
      console.log('‚ùå HF_TOKEN no configurado en Vercel');
      return res.status(200).json({
        success: false,
        response: "",
        source: 'no_token',
        error: 'Token de HuggingFace no configurado',
        debug: { instruction: 'use_frontend_fallback' }
      });
    }
    
    console.log('‚úÖ Token HF presente (primeros 10 chars):', HF_TOKEN.substring(0, 10) + '...');
    
    // ============================================
    // M√âTODO 1: HUGGINGFACE ROUTER (PRINCIPAL)
    // ============================================
    console.log('üöÄ M√©todo 1: Intentando HuggingFace Router...');
    
    let respuestaIA = null;
    let modeloUsado = null;
    let errorDetallado = null;
    
    // Lista de modelos a probar en el router
    const modelos = [
      'Qwen/Qwen2.5-7B-Instruct',
      'mistralai/Mistral-7B-Instruct-v0.2',
      'HuggingFaceH4/zephyr-7b-beta',
      'google/flan-t5-xxl'
    ];
    
    for (const modelo of modelos) {
      try {
        console.log(`üîÑ Probando modelo en router: ${modelo}`);
        
        // Endpoint del router
        const endpoint = 'https://router.huggingface.co/hf-inference/models';
        
        // Construir payload
        const payload = {
          model: modelo,
          inputs: construirPrompt(food, option, isSpoiled),
          parameters: {
            max_new_tokens: 300,
            temperature: 0.7,
            top_p: 0.9
          }
        };
        
        console.log('üì§ Enviando a router con payload:', JSON.stringify(payload).substring(0, 200) + '...');
        
        const controller = new AbortController();
        const timeout = setTimeout(() => controller.abort(), 30000);
        
        const response = await fetch(endpoint, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${HF_TOKEN}`,
            'Content-Type': 'application/json',
            'Accept': 'application/json'
          },
          body: JSON.stringify(payload),
          signal: controller.signal
        });
        
        clearTimeout(timeout);
        
        console.log(`üì• Router respondi√≥ para ${modelo}:`, {
          status: response.status,
          statusText: response.statusText,
          ok: response.ok
        });
        
        if (response.ok) {
          const data = await response.json();
          console.log(`‚úÖ Respuesta JSON para ${modelo}:`, JSON.stringify(data).substring(0, 300));
          
          // Intentar extraer texto de diferentes formatos
          let textoExtraido = null;
          
          if (Array.isArray(data) && data[0] && data[0].generated_text) {
            textoExtraido = data[0].generated_text;
          } else if (data.generated_text) {
            textoExtraido = data.generated_text;
          } else if (data[0] && typeof data[0] === 'string') {
            textoExtraido = data[0];
          } else if (typeof data === 'string') {
            textoExtraido = data;
          } else if (data.text) {
            textoExtraido = data.text;
          }
          
          if (textoExtraido && textoExtraido.length > 30) {
            console.log(`üéØ Modelo ${modelo} funcion√≥! Texto extra√≠do (primeros 100 chars):`, textoExtraido.substring(0, 100));
            respuestaIA = textoExtraido;
            modeloUsado = modelo;
            break; // ¬°√âxito!
          } else {
            console.log(`‚ö†Ô∏è Modelo ${modelo} respondi√≥ pero texto muy corto o inv√°lido:`, textoExtraido?.length || 0);
          }
        } else {
          const errorText = await response.text().catch(() => 'Sin cuerpo de error');
          console.log(`‚ùå Modelo ${modelo} fall√≥ con status ${response.status}:`, errorText.substring(0, 200));
          errorDetallado = `Router ${response.status}: ${errorText.substring(0, 100)}`;
        }
        
      } catch (modeloError) {
        console.log(`‚ö†Ô∏è Error al probar modelo ${modelo}:`, modeloError.message);
      }
    }
    
    // ============================================
    // M√âTODO 2: API INFERENCE DIRECTA (FALLBACK)
    // ============================================
    if (!respuestaIA) {
      console.log('üîÑ M√©todo 1 fall√≥, intentando M√©todo 2: API Inference directa...');
      
      try {
        // Usar un modelo peque√±o que suele estar cargado
        const modeloDirecto = 'google/flan-t5-xxl';
        const endpointDirecto = `https://api-inference.huggingface.co/models/${modeloDirecto}`;
        
        const promptDirecto = construirPromptSimple(food, option, isSpoiled);
        
        console.log('üì§ Enviando a API directa...');
        
        const controller = new AbortController();
        const timeout = setTimeout(() => controller.abort(), 20000);
        
        const response = await fetch(endpointDirecto, {
          method: 'POST',
          headers: {
            'Authorization': `Bearer ${HF_TOKEN}`,
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({
            inputs: promptDirecto,
            parameters: {
              max_new_tokens: 200,
              temperature: 0.7
            }
          }),
          signal: controller.signal
        });
        
        clearTimeout(timeout);
        
        if (response.ok) {
          const data = await response.json();
          console.log('‚úÖ API directa respondi√≥:', JSON.stringify(data).substring(0, 300));
          
          if (Array.isArray(data) && data[0] && data[0].generated_text) {
            respuestaIA = data[0].generated_text;
            modeloUsado = modeloDirecto;
          }
        }
        
      } catch (directError) {
        console.log('‚ö†Ô∏è M√©todo 2 tambi√©n fall√≥:', directError.message);
      }
    }
    
    // ============================================
    // DECIDIR QU√â RESPONDER
    // ============================================
    if (respuestaIA && respuestaIA.length > 30) {
      console.log('üéâ ¬°IA FUNCION√ì! Enviando respuesta...');
      
      const respuestaLimpia = limpiarRespuesta(respuestaIA);
      
      return res.status(200).json({
        success: true,
        response: respuestaLimpia,
        source: 'huggingface_ai',
        model: modeloUsado,
        debug: {
          timestamp: new Date().toISOString(),
          responseLength: respuestaLimpia.length,
          methodUsed: 'router_and_direct',
          food,
          option,
          isSpoiled
        }
      });
      
    } else {
      console.log('üí• Todos los m√©todos fallaron. Usando fallback frontend.');
      
      return res.status(200).json({
        success: false,
        response: "", // Vac√≠o para que frontend use su base
        source: 'all_methods_failed',
        error: errorDetallado || 'No se pudo conectar con ning√∫n servicio de IA',
        debug: {
          timestamp: new Date().toISOString(),
          food,
          option,
          isSpoiled,
          instruction: 'use_frontend_fallback_immediately'
        }
      });
    }
    
  } catch (error) {
    console.error('üí• ERROR CR√çTICO en API:', error.message, error.stack);
    
    return res.status(200).json({
      success: false,
      response: "",
      source: 'api_critical_error',
      error: String(error.message),
      debug: {
        timestamp: new Date().toISOString(),
        instruction: 'use_frontend_fallback_immediately',
        stack: error.stack?.substring(0, 200)
      }
    });
  }
}

// ============================================
// FUNCIONES AUXILIARES
// ============================================

function construirPrompt(food, option, isSpoiled) {
  if (option === 'conservation') {
    if (isSpoiled) {
      return `Eres un experto en seguridad alimentaria. Mi ${food} est√° en mal estado. ¬øQu√© debo hacer? ¬øEs seguro consumir algo de √©l? ¬øC√≥mo prevenir esto en el futuro? Responde en espa√±ol de forma pr√°ctica y concisa.`;
    } else {
      return `Eres un especialista en conservaci√≥n de alimentos. ¬øC√≥mo puedo conservar ${food} fresco por m√°s tiempo? Da consejos pr√°cticos en espa√±ol.`;
    }
  } else {
    if (isSpoiled) {
      return `Eres un chef profesional y experto en seguridad alimentaria. Tengo ${food} que parece estar en mal estado. ¬øEs seguro cocinar con √©l? ¬øQu√© alternativas sugieres? Responde en espa√±ol.`;
    } else {
      return `Eres un chef creativo. Proporciona una receta deliciosa, f√°cil y pr√°ctica usando ${food}. Responde en espa√±ol con formato claro.`;
    }
  }
}

function construirPromptSimple(food, option, isSpoiled) {
  if (option === 'conservation') {
    return isSpoiled 
      ? `Consejos para ${food} en mal estado:`
      : `C√≥mo conservar ${food}:`;
  } else {
    return isSpoiled
      ? `Alternativas para ${food} en mal estado:`
      : `Receta con ${food}:`;
  }
}

function limpiarRespuesta(texto) {
  return texto
    .replace(/\\n/g, '\n')
    .replace(/\n+/g, '\n')
    .replace(/^\s+/, '')
    .trim();
}
